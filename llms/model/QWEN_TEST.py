# %% library load
!pip install -r requirements.txt
import torch
import importlib
from runner import llm_sampling,run_inference,evaluate_results
import json
from transformers import AutoModelForCausalLM, AutoTokenizer

# %% data load
with open("/workspace/MCP_Voice_Transfer/experiments/llms/labeled data/samples.json") as f:
    samples=json.load(f)

with open("/workspace/MCP_Voice_Transfer/experiments/llms/labeled data/transfer.json") as f:
    transfer=json.load(f)
with open("/workspace/MCP_Voice_Transfer/experiments/llms/labeled data/non_memory.json") as f:
    non_memory=json.load(f)
        
print(samples)

# %% data load
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct",torch_dtype=torch.float16).to("cuda")


#%%
from prompts import unified_system_prompt6
result, pasing, elapsed = run_inference("ì•ˆë…•",unified_system_prompt6, tokenizer, model)
print("ğŸ” ì¶”ë¡  ê²°ê³¼:", result)
print("ğŸ§© íŒŒì‹±ëœ JSON:\n", pasing)
print("â±ï¸ ì²˜ë¦¬ ì‹œê°„:", elapsed, "ì´ˆ")

print(samples[3]['text'])
result, pasing, elapsed = run_inference(samples[3]['text'],unified_system_prompt6, tokenizer, model)
print("ğŸ” ì¶”ë¡  ê²°ê³¼:", result)
print("ğŸ§© íŒŒì‹±ëœ JSON:\n", pasing)
print("â±ï¸ ì²˜ë¦¬ ì‹œê°„:", elapsed, "ì´ˆ")


#%%
results_summary={}
prompt_module = importlib.import_module("prompts")
for i in range(1,7):
    prompt_name=f"unified_system_prompt{i}"
    prompt_fn = getattr(prompt_module, prompt_name, None)
    
    if prompt_fn is None:
        print(f"âŒ {prompt_name} í•¨ìˆ˜ ì—†ìŒ")
        continue
    
    print(f"âœ… ì‹¤í–‰ ì¤‘: {prompt_name}")
    evaluation =llm_sampling(model,tokenizer, samples, prompt_fn)
    results_summary[prompt_name] = evaluation

 
 
#%% ASSESMENT

 
# ì „ì²´ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
print("\nğŸ“Š ì „ì²´ í‰ê°€ ìš”ì•½")
for name, evaluation in results_summary.items():
    print(f"\nğŸ”¹ {name}")
    for metric, value in evaluation.items():
        print(f"  - {metric}: {value}")


# %%
